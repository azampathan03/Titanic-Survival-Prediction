Titanic Survival Prediction
The sinking of theÂ **RMS Titanic**Â in 1912 remains one of the most tragic maritime disasters in history. WithÂ **1,502 lives lost**Â out ofÂ **2,224 passengers and crew**, the event has been widely studied, including in data science and machine learning. Predicting survival on the Titanic is a classic classification problem used to teach predictive modeling.

Web Overview
<img width="960" height="540" alt="titanic app1" src="https://github.com/user-attachments/assets/b5234384-97dc-45d9-a5ce-576186db0ca2" />


## **1. Understanding the Dataset**

The Titanic dataset contains information about passengers, including:

- **PassengerId**Â â€“ Unique identifier
- **Survived**Â â€“ 0 (No), 1 (Yes)Â *(Target Variable)*
- **Pclass**Â â€“ Ticket class (1st, 2nd, 3rd)
- **Name**Â â€“ Passenger name
- **Sex**Â â€“ Male or Female
- **Age**Â â€“ Age in years
- **SibSp**Â â€“ Number of siblings/spouses aboard
- **Parch**Â â€“ Number of parents/children aboard
- **Ticket**Â â€“ Ticket number
- **Fare**Â â€“ Passenger fare
- **Cabin**Â â€“ Cabin number
- **Embarked**Â â€“ Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)


---

## **2. Key Factors Affecting Survival**

### **A. Socio-Economic Status (Pclass)**

- **1st class passengers**Â had a higher survival rate (**~63%**) due to proximity to lifeboats.
- **3rd class passengers**Â had the lowest survival rate (**~24%**).

### **B. Gender (Sex)**

- **Female passengers**Â had aÂ **~74%**Â survival rate due to the "women and children first" protocol.
- **Male passengers**Â had only aÂ **~19%**Â survival rate.

### **C. Age (Age)**

- **Children (<10 years)**Â had a higher chance of survival.
- **Elderly passengers**Â had lower survival rates.

### **D. Family Size (SibSp & Parch)**

- Passengers withÂ **1-2 family members**Â had better survival odds.
- Large families (>3) struggled to evacuate together.

---

## **3. Machine Learning Approach**

### **A. Data Preprocessing**

- **Handling missing values**Â (e.g., filling missingÂ **`Age`**Â with median).
- **Converting categorical data**Â (e.g.,Â **`Sex`**Â andÂ **`Embarked`**Â into numerical values).
- **Feature engineering**Â (e.g., extracting titles fromÂ **`Name`**, groupingÂ **`Age`**Â into bins).

### **B. Model Selection**

Common algorithms used:

1. **Logistic Regression**Â (Baseline model)
2. **Random Forest**Â (Handles non-linear relationships well)
3. **Gradient Boosting (XGBoost)**Â (High accuracy)
4. **Support Vector Machines (SVM)**Â (Works well with structured data)

### **C. Model Evaluation**

Metrics used:

- **Accuracy**Â (Overall correctness)
- **Precision & Recall**Â (Minimizing false positives/negatives)
- **F1-Score**Â (Balanced measure)
- **ROC-AUC**Â (Performance across thresholds)

---

## **4. Example Prediction (Random Forest)**

A trained model might predict survival based on input features:

| **Feature** | **Passenger 1 (Survived)** | **Passenger 2 (Did Not Survive)** |
| --- | --- | --- |
| **Pclass** | 1 (Upper) | 3 (Lower) |
| **Sex** | Female | Male |
| **Age** | 28 | 45 |
| **Fare** | Â£50 | Â£8 |
| **Embarked** | C (Cherbourg) | S (Southampton) |
| **Prediction** | âœ…Â **Survived (95%)** | âŒÂ **Died (87%)** |

---

---

## **5. Conclusion**

- **Women, children, and upper-class passengers**Â had the best survival odds.
- **Machine learning models**Â can predict survival withÂ **~80% accuracy**Â using the right features.
- **Feature engineering**Â (e.g., family size, title extraction) improves predictions.

This analysis helps us understand historical biases in survival and serves as a foundation for classification problems in data science.

**ğŸ“© Connect**

[LinkedInÂ ](https://www.linkedin.com/in/azamhussain03/)

